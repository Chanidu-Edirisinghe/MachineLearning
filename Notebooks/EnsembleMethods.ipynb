{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation Steps of Bagging\n",
    "\n",
    "Step 1: Multiple subsets are created from the original data set with equal tuples, selecting observations with replacement.\n",
    "\n",
    "Step 2: A base model is created on each of these subsets.\n",
    "\n",
    "Step 3: Each model is learned in parallel with each training set and independent of each other.\n",
    "\n",
    "Step 4: The final predictions are determined by combining the predictions from all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9919354838709677\n",
      "Testing Accuracy: 0.9259259259259259\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "random_state=42)\n",
    "\n",
    "# Initialize the base classifier (in this case, a decision tree)\n",
    "base_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Initialize the BaggingClassifier\n",
    "bagging_classifier = BaggingClassifier(estimator=base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the BaggingClassifier\n",
    "bagging_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, bagging_classifier.predict(X_train))\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the dataset and assign equal weight to each of the data point.\n",
    "\n",
    "Provide this as input to the model and identify the wrongly classified data points.\n",
    "\n",
    "Increase the weight of the wrongly classified data points.\n",
    "\n",
    "if (got required results) \n",
    "\n",
    "  Goto step 5 \n",
    "\n",
    "else \n",
    "\n",
    "  Goto step 2 \n",
    "\n",
    "End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost - Training Accuracy: 1.0\n",
      "Adaboost - Testing Accuracy: 0.9629629629629629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier  # For Classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Now we will use decision tree as a base estimator, you can use any ML learner as base estimator if it accepts sample weight \n",
    "dt = DecisionTreeClassifier() \n",
    "clf = AdaBoostClassifier(n_estimators=100, estimator=dt, learning_rate=1)\n",
    "\n",
    "# training the model on the training set\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# calculate and print training and testing accuracy\n",
    "train_accuracy = accuracy_score(y_train, clf.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "\n",
    "print(\"Adaboost - Training Accuracy:\", train_accuracy)\n",
    "print(\"Adaboost - Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting - Training Accuracy: 1.0\n",
      "Gradient Boosting - Test Accuracy: 1.0\n",
      "XGBoost - Training Accuracy: 1.0\n",
      "XGBoost - Test Accuracy: 0.9814814814814815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:07:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "gb_train_accuracy = accuracy_score(y_train, gb_clf.predict(X_train))\n",
    "gb_test_accuracy = accuracy_score(y_test, gb_clf.predict(X_test))\n",
    "\n",
    "print(\"Gradient Boosting - Training Accuracy:\", gb_train_accuracy)\n",
    "print(\"Gradient Boosting - Test Accuracy:\", gb_test_accuracy)\n",
    "\n",
    "#-------------------------#\n",
    "# XGBoost Classifier\n",
    "# Make sure xgboost is installed: pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_clf = XGBClassifier(n_estimators=100, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "xgb_train_accuracy = accuracy_score(y_train, xgb_clf.predict(X_train))\n",
    "xgb_test_accuracy = accuracy_score(y_test, xgb_clf.predict(X_test))\n",
    "\n",
    "print(\"XGBoost - Training Accuracy:\", xgb_train_accuracy)\n",
    "print(\"XGBoost - Test Accuracy:\", xgb_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Models: Training multiple models (level-0 models) on the same dataset.\n",
    "\n",
    "Meta-Model: Training a new model (level-1 or meta-model) to combine the predictions of the base models. Using the predictions of the base models as input features for the meta-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Training Accuracy: 0.97\n",
      "Stacking Classifier Testing Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "('svm', SVC(probability=True, random_state=42))\n",
    "]\n",
    "\n",
    "# Define meta-model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Initialize and train the StackingClassifier\n",
    "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_train, stacking_clf.predict(X_train))\n",
    "print(f\"Stacking Classifier Training Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Stacking Classifier Testing Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
